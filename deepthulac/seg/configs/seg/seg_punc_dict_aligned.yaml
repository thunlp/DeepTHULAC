# CUDA_VISIBLE_DEVICES=0 python deepthulac/seg/train.py --config_path deepthulac/seg/configs/seg/seg_punc_dict_aligned.yaml

# 模型相关参数
# CUDA_VISIBLE_DEVICES=7 python deepthulac/seg/train.py
# accelerate launch --gpu_ids 0,1,2,3 --num_processes 4 --mixed_precision bf16 deepthulac/seg/train.py
pretrained_bert_model: bert-base-chinese # bert-base-chinese #./pretrained # 模型路径或huggingface的模型
heads:
  - seg
  - punc
seg_labels:
  - B
  - M
  - E
  - S
punc_labels:
  - B
  - M
  - E
  - S
head_config:
  use_crf: False
  layers_num: 1

# 训练相关
saved_path: output/seg_punc_dict_aligned
learning_rate: 5.0e-06
warmup_steps: 0
# eval_steps: 7200
weight_decay: 0.01
clip_grad: 5
batch_size: 32 # 128 64*4

# shuffle_samples 会将所有数据集的样本按repeat_times重复，混合后合并为一个数据集进行训练
# shuffle_batches 会将所有数据集的batch按repeat_times重复，混合后训练，每个训练集都会被完整地训练repeat_times次
# continuous_batches 会对各个数据集的batch连续训练continuous_batches个batch后轮换训练，最先训完的数据集只训练一遍，其余训练集可能训练多遍
data_strategy: continuous_batches
train_datasets:
  # dataset_name:
  #   dir: 数据集所在文件夹，可选，默认为./data，文件路径为{dir}/{dataset_name}_train_{num}.txt
  #   file_order_reverse: 当数据集是一个文件夹，文件夹下的文件是否逆序访问。对于punc的rm和rs如果同时训练时，一个顺序一个逆序可以防止训练相同数据
  #   extra_info: 字典数据是否使用边界信息，可选，默认为False。实验结果表明用extra_info更好
  #   continuous_batches: （data_strategy=continuous_batches时有效且必填）此数据集会连续训练多少个batch才轮换到其他dataset
  #   repeat_times: （data_strategy=shuffle_*时有效且必填）此数据集在一个epoch中会被重复使用多少遍
  # TODO: 写一个samples_num
  punc_baidubaike-rs:
    dir: data
    continuous_batches: 5
    file_order_reverse: False
  punc_baidubaike-rm:
    dir: data
    continuous_batches: 5
    file_order_reverse: True
  punc_ours: # for alignment
    dir: data
    continuous_batches: 1
  punc_ours-rm: # for alignment
    dir: data
    continuous_batches: 1

  dict_wantwords-shuffle: # 词典这玩意少用？太依赖数据选择
    dir: data
    continuous_batches: 5
    extra_info: False
  seg_ours-rm:  # for data augmentation
    dir: data
    continuous_batches: 3
  seg_ours:
    dir: data
    continuous_batches: 7

dev_datasets:
  seg_ours:
    dir: data

part_data: False
