# 模型相关参数
# accelerate launch --gpu_ids 0,1,2,3,4,5,6,7 --num_processes 8 --mixed_precision fp16 deepthulac/seg/train.py --config_path=deepthulac/seg/configs/seg/seg_dict_pretrain.yaml
pretrained_bert_model: /data03/private/chengzhili/pretrain/output/2023-06-09_21-23-00/save
load_seg_pretrain: "/data03/private/chengzhili/segmentation/output/seg/2023-06-24_03-48-13 ok"
heads:
  - seg
  #- punc
seg_labels:
  - B
  - M
  - E
  - S
#punc_labels:
#  - B
#  - M
#  - E
#  - S
head_config:
  use_crf: False
  layers_num: 1

# 训练相关
saved_path: output/seg_dict_pretrain
learning_rate: 1.0e-07
warmup_steps: 0
# eval_steps: 7200
weight_decay: 0
clip_grad: 1
batch_size: 36 # 128 64*4

# shuffle_samples 会将所有数据集的样本按repeat_times重复，混合后合并为一个数据集进行训练
# shuffle_batches 会将所有数据集的batch按repeat_times重复，混合后训练，每个训练集都会被完整地训练repeat_times次
# continuous_batches 会对各个数据集的batch连续训练continuous_batches个batch后轮换训练，最先训完的数据集只训练一遍，其余训练集可能训练多遍
data_strategy: shuffle_batches # continuous_batches

label_mode: ME
train_datasets:
  # dataset_name:
  #   dir: 数据集所在文件夹，可选，默认为./data，文件路径为{dir}/{dataset_name}_train_{num}.txt
  #   file_order_reverse: 当数据集是一个文件夹，文件夹下的文件是否逆序访问。对于punc的rm和rs如果同时训练时，一个顺序一个逆序可以防止训练相同数据
  #   extra_info: 字典数据是否使用边界信息，可选，默认为False。实验结果表明用extra_info更好
  #   continuous_batches: （data_strategy=continuous_batches时有效且必填）此数据集会连续训练多少个batch才轮换到其他dataset
  #   repeat_times: （data_strategy=shuffle_*时有效且必填）此数据集在一个epoch中会被重复使用多少遍
  # TODO: 写一个samples_num
  # punc_baidubaike-rs:
  #   dir: data
  #   continuous_batches: 1
  #   file_order_reverse: False
  # punc_baidubaike-rm:
  #   dir: data
  #   continuous_batches: 1
  #   file_order_reverse: True

    
  #punc_ours: # 别别别，拉拉拉
  #  dir: data
  #  continuous_batches: 1
  #punc_ours-rm: # 别别别，拉拉拉
  #  dir: data
  #  continuous_batches: 1
  # TODO: 每个单独batch size 每个单独learning rate
  # TODO: 多以单字手动造例子，把把把->把 把 把 很好地正则约束；random char random char random char 全分开
  # dict_wantwords-shuffle: # 变差
  #   dir: data
  #   continuous_batches: 1
  #   repeat_times: 1
  #   extra_info: False ##??
  seg_zuozhuan: # 别别别，拉拉拉
    dir: data
    continuous_batches: 1
    repeat_times: 1
  dict_wantwords: # 别别别，拉拉拉
    dir: data
    continuous_batches: 1
    repeat_times: 1
  #seg_zuozhuan-rm: # 别别别，拉拉拉
  #  dir: data
  #  continuous_batches: 1
  #  repeat_times: 1
  #seg_designed: # 变差
  #  dir: data
  #  continuous_batches: 1
  #seg_ours-rm: # 别别别，拉拉拉
  #  dir: data
  #  continuous_batches: 1
  #seg_ours:
  #  dir: data
  #  continuous_batches: 1

dev_datasets:
  seg_ours:
    dir: data

part_data: False
